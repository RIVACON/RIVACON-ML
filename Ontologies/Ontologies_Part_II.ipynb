{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/logo.png\" width=\"500\" /></center>\n",
    "\n",
    "# LLMs and Ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Context\n",
    "\n",
    "As referenced in our previous notebook about ontologies ([Ontologies Part I](Ontologies/Ontologies_Part_I.ipynb)), ontologies provide a semantic way to organise your data. More explicitly, it provies a structured and formal representation of knowledge, defining concepts, relationships, and rules within a specific domain to enable interoperability, reasoning, and data integration. As we showed before, one can use SPARQL to query the RDF Triples format of an ontology and and instance thereof. This of course requires knowledge on the structure of your ontology to a degree.\n",
    "\n",
    "Given that Large Language Models (LLMs) have improved to a degree where they are quite proficient in extracting information from a document ([Integrate and use GPT with Python](GPT_and_Python/GPT_and_Python.ipynb)), how can we leverage an ontology to further improve our queries by using an ontology? We provide an example here which shows on a small scale with a simple ontology, how it can provide semantic and structured information to the LLM, helping it to provide clearer responses within context.\n",
    "\n",
    "\n",
    "**Please note that you will need access to an LLM to run this notebook. Here we have used OpenAI for which you will need a license key. Another LLM may be used however with possible syntax changes. The core methodology is the same.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Python Implementation\n",
    "\n",
    "The steps needed to implement our approach is as follows:\n",
    "\n",
    "1. Extract ontology data as txt\n",
    "2. Feed to LLM as context\n",
    "3. Query LLM and get response\n",
    "\n",
    "where the biggest obstacle is transforming the ontology into a format that is LLM friendly, i.e., text. \n",
    "To do so, we need to ensure that the following concepts are captured in the text:\n",
    "\n",
    "1. Classes, properties, relationships\n",
    "2. Inheritance of subclasses\n",
    "3. Instances of classes\n",
    "4. Data values of instances\n",
    "\n",
    "And so we keep these in mind while defining the function to convert an ontology into text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LLM friendly format\n",
    "\n",
    "We must keep in mind that an LLM such as OpenAI takes in as input, regular text.\n",
    "Therefore, to make the most of the ontology, which normally might look something like this in RDF or OWL format:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; <owl:ObjectProperty rdf:about=\"#DividedInto\">  \n",
    "&nbsp;&nbsp;&nbsp;   <rdfs:domain rdf:resource=\"#ReportingFramework\"/>  \n",
    "&nbsp;&nbsp;&nbsp;   <rdfs:range rdf:resource=\"#Category\"/>  \n",
    "&nbsp;&nbsp;&nbsp; </owl:ObjectProperty>  \n",
    "\n",
    "it needs to be converted into something like:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; **A _ReportingFramework_ is a class that has the property that it is _DivivdedInto_ some _Categories_.**\n",
    "\n",
    "Therefore, we need to define a function like **extract_ontology_info** (below) that will extract all the classes into proper sentences.\n",
    "It also needs to make proper sentences defining the relationships and properties.\n",
    "We aim for something like:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; Class Hierarchy  \n",
    "&nbsp;&nbsp;&nbsp; - **ReportingFramework** is a subclass of Thing.  \n",
    "&nbsp;&nbsp;&nbsp; - **Category** is a subclass of Thing.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp; Properties and Inheritance\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; Class: **ReportingFramework**  \n",
    "&nbsp;&nbsp;&nbsp; Directly Defined Properties:  \n",
    "&nbsp;&nbsp;&nbsp; - Object Properties: DividedInto  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp; Class: **Category**  \n",
    "&nbsp;&nbsp;&nbsp; Directly Defined Properties:   \n",
    "&nbsp;&nbsp;&nbsp; - Object Properties: DividedIntoSub  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES\n",
    "\n",
    "import openai  # Replace with your LLM provider\n",
    "from openai import OpenAI\n",
    "from owlready2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key: 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\n",
      "*** Please do not share this key! Remove this output if in public; here for debug. ***\n"
     ]
    }
   ],
   "source": [
    "## Configure the OpenAI API Licence key using an explicit LOCAL file\n",
    "#tmp_file = open(\"openaikey.txt\", \"r\")\n",
    "#tmp = tmp_file.readline()\n",
    "## The licence key ist stored in the second line\n",
    "#the_key = tmp_file.readline().strip()\n",
    "#tmp_file.close()\n",
    "\n",
    "#Please replace the string with your license key\n",
    "the_key = \"123456789_REPLACE\"\n",
    "print(f\"Using key: {the_key}\")\n",
    "print(\"*** Please do not share this key! Remove this output if in public; here for debug. ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = the_key \n",
    "client = OpenAI(api_key=the_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION DEFINITIONS\n",
    "\n",
    "# Function to extract knowledge from ontology\n",
    "def extract_ontology_info(onto, save_knowledge_as_txt = False):\n",
    "    \"\"\"\n",
    "    Converts an OWL ontology into structured text for an LLM.\n",
    "    - Captures class hierarchy and inheritance.\n",
    "    - Lists properties, including inherited ones.\n",
    "    - Records individualsinstances and their values.\n",
    "\n",
    "    params:\n",
    "    - onto: Owlready2 ontology format\n",
    "    - save_knowledge_as_txt: boolean to save converted ontology as txt\n",
    "    \n",
    "    \"\"\"\n",
    "    text_output = \"\"\n",
    "\n",
    "\n",
    "    # Extract all object and data properties\n",
    "    all_object_properties = list(onto.object_properties())\n",
    "    all_data_properties = list(onto.data_properties())\n",
    "\n",
    "    # Extract class hierarchy\n",
    "    class_hierarchy = {cls.name: [p.name for p in cls.is_a if isinstance(p, ThingClass)]\n",
    "                       for cls in onto.classes()}\n",
    "\n",
    "    # Describe the class hierarchy\n",
    "    text_output += \"## Class Hierarchy\\n\"\n",
    "    for cls, parents in class_hierarchy.items():\n",
    "        if parents:\n",
    "            text_output += f\"- **{cls}** is a subclass of {', '.join(parents)}.\\n\"\n",
    "        else:\n",
    "            text_output += f\"- **{cls}** is a top-level class.\\n\"\n",
    "\n",
    "    text_output += \"\\n## Properties and Inheritance\\n\"\n",
    "\n",
    "    # Extract properties per class\n",
    "    class_properties = {}\n",
    "    for cls in onto.classes():\n",
    "        own_props = {\"Object Properties\": [], \"Data Properties\": []}\n",
    "        inherited_props = {}\n",
    "\n",
    "        # Find properties that apply to this class\n",
    "        for prop in all_object_properties + all_data_properties:\n",
    "            if cls in prop.domain:\n",
    "                if isinstance(prop, ObjectPropertyClass):\n",
    "                    own_props[\"Object Properties\"].append(prop.name)\n",
    "                elif isinstance(prop, DataPropertyClass):\n",
    "                    own_props[\"Data Properties\"].append(prop.name)\n",
    "\n",
    "        # Store properties for later instance processing\n",
    "        class_properties[cls] = own_props\n",
    "\n",
    "        # Check inherited properties from parent classes\n",
    "        for parent in cls.is_a:\n",
    "            if isinstance(parent, ThingClass):  \n",
    "                for prop in all_object_properties + all_data_properties:\n",
    "                    if parent in prop.domain and prop.name not in own_props:\n",
    "                        inherited_props[prop] = parent.name\n",
    "\n",
    "        text_output += f\"\\n### Class: {cls.name}\\n\"\n",
    "        if own_props[\"Object Properties\"] or own_props[\"Data Properties\"]:\n",
    "            text_output += \"**Directly Defined Properties:**\\n\"\n",
    "            if own_props[\"Object Properties\"]:\n",
    "                text_output += f\"- Object Properties: {', '.join(own_props['Object Properties'])}\\n\"\n",
    "            if own_props[\"Data Properties\"]:\n",
    "                text_output += f\"- Data Properties: {', '.join(own_props['Data Properties'])}\\n\"\n",
    "        \n",
    "        if inherited_props:\n",
    "            text_output += \"**Inherited Properties:**\\n\"\n",
    "            for prop, parent in inherited_props.items():\n",
    "                text_output += f\"- {prop.name} (from {parent})\\n\"\n",
    "\n",
    "    text_output += \"\\n## Instances and Their Values\\n\"\n",
    "\n",
    "    # Extract all individuals (instances)\n",
    "    for cls in onto.classes():\n",
    "        instances = list(cls.instances())\n",
    "        if instances:\n",
    "            text_output += f\"\\n### Instances of {cls.name}:\\n\"\n",
    "            for inst in instances:\n",
    "                text_output += f\"- **Instance:** {inst.name}\\n\"\n",
    "\n",
    "                # Extract direct data properties\n",
    "                for prop in all_data_properties:\n",
    "                    if inst in prop.domain:\n",
    "                        values = getattr(inst, prop.name, None)\n",
    "                        if values:\n",
    "                            values = [str(v) for v in values] if isinstance(values, list) else [str(values)]\n",
    "                            text_output += f\"  - {prop.name}: {', '.join(values)}\\n\"\n",
    "\n",
    "                # Extract object properties\n",
    "                for prop in all_object_properties:\n",
    "                    if inst in prop.domain:\n",
    "                        values = getattr(inst, prop.name, None)\n",
    "                        if values:\n",
    "                            values = [str(v) for v in values] if isinstance(values, list) else [str(values)]\n",
    "                            text_output += f\"  - {prop.name}: {', '.join(values)}\\n\"\n",
    "\n",
    "                # Check for FunctionalProperty relationships (like IsInstrument)\n",
    "                for prop in all_object_properties:\n",
    "\n",
    "                    if FunctionalProperty in prop.is_a and cls in prop.domain:\n",
    "                        #print('Got Here 2')\n",
    "                        linked_entity = getattr(inst, prop.name, None)\n",
    "                        if linked_entity:\n",
    "                            text_output += f\"  - **Linked via {prop.name}:** {linked_entity.name} (inherits its properties)\\n\"\n",
    "                            # Inherit values from linked instance\n",
    "                            for linked_prop in class_properties.get(type(linked_entity), {}).get(\"Data Properties\", []):\n",
    "                                linked_value = getattr(linked_entity, linked_prop, None)\n",
    "                                if linked_value:\n",
    "                                    text_output += f\"    - Inherited {linked_prop}: {linked_value}\\n\"\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    text_output += '-------\\n'\n",
    "    for inst in onto.individuals():\n",
    "        text_output += f\"- Instance: {inst.name} (Class: {inst.is_a[0].name})\\n\"\n",
    "        # Extract data property values\n",
    "        for prop in onto.data_properties():\n",
    "            if prop in inst.get_properties():\n",
    "                values = [str(val) for val in prop[inst]]\n",
    "                text_output += f\"  - {prop.name}: {', '.join(values)}\\n\"\n",
    "\n",
    "\n",
    "    knowledge = text_output #f\"Classes: {classes}\\nProperties: {properties}\\nRelations: {relations}\\nInstances: {instances}\"\n",
    "    if save_knowledge_as_txt:\n",
    "        print(knowledge)\n",
    "        f  = open('part_ii/knowledge_feed.txt', 'w+')\n",
    "        f.write(knowledge)\n",
    "        f.close()\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to query the LLM\n",
    "def query_llm(question, knowledge):\n",
    "    response = client.chat.completions.create(#openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",#\"gpt-3.5-turbo\",#\"gpt-4\",  # Change based on your LLM\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in ontologies.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Ontology knowledge:\\n{knowledge}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "    )\n",
    "    #return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main function\n",
    "def main(onto, save_knowledge_as_txt = False, LOGFILE=True, single_question=''):\n",
    "    knowledge = extract_ontology_info(onto, save_knowledge_as_txt)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if single_question != '':\n",
    "            question = single_question\n",
    "            answer = query_llm(question, knowledge)\n",
    "            print(\"Answer:\", answer)\n",
    "            break\n",
    "        \n",
    "        question = input(\"Ask a question about the ontology (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = query_llm(question, knowledge)\n",
    "        print(\"Answer:\", answer)\n",
    "\n",
    "        if LOGFILE:\n",
    "            f  = open('part_ii/OUTPUT_LOG.txt', 'a+')\n",
    "            f.write('------\\n')\n",
    "            f.write('Question: ' +question)\n",
    "            f.write('\\n')\n",
    "            f.write('\\n')\n",
    "            f.write('Answer: ' +answer)\n",
    "            f.write('\\n')\n",
    "            f.write('------\\n')\n",
    "            f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Query the LLM\n",
    "\n",
    "In the wrappepr function we defined to query the LLM, you will notice that we preface all queries by defining the role of the LLM: *\"You are an expert in ontologies.\"* aas well as feeding it first, the context of ontology and then followed by the question.\n",
    "\n",
    "    f\"Ontology knowledge:\\n{knowledge}\\n\\nQuestion: {question}\"\n",
    "\n",
    "This query prompting style allows the LLM to better understand the context.\n",
    "We note, however, that in this simple example, we are feeding the LLM the same context with every query which for substantial products is inefficient but works here as an example. Ideally, one would a) fine-tune their LLM and/or b) tokenize a database for Retrieval Augmented Generation (RAG) in order to more efficiently query the LLM. These of course require computational resources which are not explored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology\n",
    "onto_fname = \"onto_repo/instances_SCR_onto.owl\"\n",
    "onto = get_ontology(onto_fname).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Class Hierarchy\n",
      "- **ReportingFramework** is a subclass of Thing.\n",
      "- **Category** is a subclass of Thing.\n",
      "- **Subcategory** is a subclass of Category.\n",
      "- **ComputingModel** is a subclass of Thing.\n",
      "- **Metric** is a subclass of Thing.\n",
      "- **Dataset** is a subclass of Thing.\n",
      "- **Indicator** is a subclass of Thing.\n",
      "- **Datasource** is a subclass of Thing.\n",
      "- **Institution** is a subclass of Thing.\n",
      "- **Portfolio** is a subclass of Thing.\n",
      "- **Position** is a subclass of Thing.\n",
      "- **Instrument** is a subclass of Thing.\n",
      "- **Property** is a subclass of Instrument.\n",
      "- **Bonds** is a subclass of Instrument.\n",
      "\n",
      "## Properties and Inheritance\n",
      "\n",
      "### Class: ReportingFramework\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: DividedInto\n",
      "\n",
      "### Class: Category\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: DividedIntoSub\n",
      "\n",
      "### Class: Subcategory\n",
      "**Inherited Properties:**\n",
      "- DividedIntoSub (from Category)\n",
      "\n",
      "### Class: ComputingModel\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: HasCategory, DependentVariableMetric, DependentVariableIndicator\n",
      "\n",
      "### Class: Metric\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: ObtainedFromModel, ObtainedFromDataset, DependentVariableIndicatorM\n",
      "\n",
      "### Class: Dataset\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: ComesFromDatasource\n",
      "\n",
      "### Class: Indicator\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: UsesDataset\n",
      "\n",
      "### Class: Datasource\n",
      "\n",
      "### Class: Institution\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: OwnsDataset, DataProvider, HasPortfolio\n",
      "\n",
      "### Class: Portfolio\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: HasPositions\n",
      "\n",
      "### Class: Position\n",
      "**Directly Defined Properties:**\n",
      "- Object Properties: IsInstrument\n",
      "\n",
      "### Class: Instrument\n",
      "\n",
      "### Class: Property\n",
      "\n",
      "### Class: Bonds\n",
      "\n",
      "## Instances and Their Values\n",
      "\n",
      "### Instances of ReportingFramework:\n",
      "- **Instance:** SCR\n",
      "- **Instance:** Risk_Modules\n",
      "\n",
      "### Instances of Category:\n",
      "- **Instance:** Risk_Modules\n",
      "- **Instance:** operational\n",
      "- **Instance:** counter_party_default\n",
      "- **Instance:** life_underwriting\n",
      "- **Instance:** health_underwriting\n",
      "- **Instance:** non_life_underwriting\n",
      "- **Instance:** market\n",
      "\n",
      "### Instances of Subcategory:\n",
      "- **Instance:** operational\n",
      "- **Instance:** counter_party_default\n",
      "- **Instance:** life_underwriting\n",
      "- **Instance:** health_underwriting\n",
      "- **Instance:** non_life_underwriting\n",
      "- **Instance:** market\n",
      "\n",
      "### Instances of ComputingModel:\n",
      "- **Instance:** Market_Risk_Module\n",
      "\n",
      "### Instances of Metric:\n",
      "- **Instance:** Equity_Risk_sub_module\n",
      "- **Instance:** Property_Risk_sub_module\n",
      "- **Instance:** Interest_Rate_Risk_sub_module\n",
      "- **Instance:** Spread_Risk_sub_module\n",
      "\n",
      "### Instances of Dataset:\n",
      "- **Instance:** book_assets.csv\n",
      "\n",
      "### Instances of Indicator:\n",
      "- **Instance:** immovable_property\n",
      "- **Instance:** own_funds\n",
      "\n",
      "### Instances of Datasource:\n",
      "- **Instance:** Internal_Department\n",
      "\n",
      "### Instances of Institution:\n",
      "- **Instance:** best_insurance_firm\n",
      "  - **Linked via HasPortfolio:** best_portfolio (inherits its properties)\n",
      "\n",
      "### Instances of Portfolio:\n",
      "- **Instance:** best_portfolio\n",
      "\n",
      "### Instances of Position:\n",
      "- **Instance:** position_3\n",
      "  - **Linked via IsInstrument:** propertyC (inherits its properties)\n",
      "- **Instance:** position_2\n",
      "  - **Linked via IsInstrument:** propertyB (inherits its properties)\n",
      "- **Instance:** position_1\n",
      "  - **Linked via IsInstrument:** propertyA (inherits its properties)\n",
      "- **Instance:** position_4\n",
      "  - **Linked via IsInstrument:** propertyD (inherits its properties)\n",
      "\n",
      "### Instances of Instrument:\n",
      "- **Instance:** propertyA\n",
      "- **Instance:** propertyB\n",
      "- **Instance:** propertyC\n",
      "- **Instance:** propertyD\n",
      "\n",
      "### Instances of Property:\n",
      "- **Instance:** propertyA\n",
      "- **Instance:** propertyB\n",
      "- **Instance:** propertyC\n",
      "- **Instance:** propertyD\n",
      "-------\n",
      "- Instance: SCR (Class: ReportingFramework)\n",
      "  - ReportingBody: European Insurance and Occupational Pensions Authority\n",
      "  - ReferenceDate: 1 July 2016\n",
      "- Instance: Risk_Modules (Class: Category)\n",
      "- Instance: operational (Class: Subcategory)\n",
      "- Instance: counter_party_default (Class: Subcategory)\n",
      "- Instance: life_underwriting (Class: Subcategory)\n",
      "- Instance: health_underwriting (Class: Subcategory)\n",
      "- Instance: non_life_underwriting (Class: Subcategory)\n",
      "- Instance: market (Class: Subcategory)\n",
      "- Instance: best_insurance_firm (Class: Institution)\n",
      "- Instance: book_assets.csv (Class: Dataset)\n",
      "- Instance: Internal_Department (Class: Datasource)\n",
      "- Instance: best_portfolio (Class: Portfolio)\n",
      "- Instance: position_3 (Class: Position)\n",
      "- Instance: position_2 (Class: Position)\n",
      "- Instance: position_1 (Class: Position)\n",
      "- Instance: position_4 (Class: Position)\n",
      "- Instance: propertyA (Class: Property)\n",
      "  - FltValue: 10000000.0\n",
      "  - ValueDim: euro\n",
      "- Instance: propertyB (Class: Property)\n",
      "  - FltValue: 1000000.0\n",
      "  - ValueDim: euro\n",
      "- Instance: propertyC (Class: Property)\n",
      "  - FltValue: 100000.0\n",
      "  - ValueDim: euro\n",
      "- Instance: propertyD (Class: Property)\n",
      "  - FltValue: 10000.0\n",
      "  - ValueDim: euro\n",
      "- Instance: Market_Risk_Module (Class: ComputingModel)\n",
      "- Instance: Equity_Risk_sub_module (Class: Metric)\n",
      "  - Article: 168, 171, 169, 170, 173, 172\n",
      "- Instance: Property_Risk_sub_module (Class: Metric)\n",
      "  - Article: 174\n",
      "  - FltValue: 2777500.0\n",
      "  - ValueDim: euro\n",
      "- Instance: Interest_Rate_Risk_sub_module (Class: Metric)\n",
      "  - Article: 165, 167, 166\n",
      "- Instance: Spread_Risk_sub_module (Class: Metric)\n",
      "  - Article: 180, 178, 179, 175, 177, 181, 176\n",
      "- Instance: immovable_property (Class: Indicator)\n",
      "  - FltValue: 11110000.0\n",
      "  - ValueDim: euro\n",
      "- Instance: own_funds (Class: Indicator)\n",
      "  - FltValue: 1000000000000.0\n",
      "  - ValueDim: euro\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the ontology (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you want to keep asking questions continuously\n",
    "main(onto, save_knowledge_as_txt= True, LOGFILE= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 More queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The instance of a metric obtained from a computing model is:\n",
      "\n",
      "- **Metric:**\n",
      "  - **Instance:** Property_Risk_sub_module\n",
      "    - **ObtainedFromModel:** Market_Risk_Module\n",
      "\n",
      "The computing model associated is:\n",
      "\n",
      "- **Computing Model:**\n",
      "  - **Instance:** Market_Risk_Module\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\" Use only the knowledge provided in the given ontology. What are all the instances of a metric obtained from a computing model and which computing model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The metrics used to calculate market risk are:\n",
      "\n",
      "- **Equity_Risk_sub_module**\n",
      "- **Interest_Rate_Risk_sub_module**\n",
      "- **Spread_Risk_sub_module**\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Which metrics are used to calculate market risk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The variables used to calculate various metrics are:\n",
      "\n",
      "1. **DependentVariableMetric** from ComputingModel.\n",
      "2. **ObtainedFromModel** from Metric.\n",
      "3. **ObtainedFromDataset** from Metric.\n",
      "4. **DependentVariableIndicatorM** from Metric.\n",
      "5. **UsesDataset** from Indicator. \n",
      "\n",
      "These properties connect metrics to their relevant models and datasets.\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Which variables are used to calculate various metrics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The reporting body is the  \n",
      "European Insurance and  \n",
      "Occupational Pensions Authority.\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Which reporting body supervises the corresponding reporting framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You can obtain the data to calculate property risk  \n",
      "from the **Dataset** instance: book_assets.csv.  \n",
      "This dataset is linked to the necessary metrics  \n",
      "to evaluate property risk.\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Where can one obtain the data to calculate property risk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The dataset \"book_assets.csv\" is controlled by  \n",
      "the institution \"best_insurance_firm\" through the  \n",
      "property \"OwnsDataset\".\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Who controls this dataset?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The available institution is:\n",
      "\n",
      "- best_insurance_firm\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. Which institutions are available?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: There are four positions: position_1, position_2,  \n",
      "position_3, and position_4.\n"
     ]
    }
   ],
   "source": [
    "main(onto, single_question=\"Use only the knowledge provided in the given ontology. How many positions does it have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use the LLM to extract ontological info from new documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: - **Instance:** Equity_Risk_Sub_Module (Class: Metric)\n",
      "  - Article: 168\n",
      "\n",
      "- **Linked to:** Market_Risk_Module (Class: ComputingModel)\n",
      "\n",
      "- **Instance:** European_Parliament (Class: Institution)\n",
      "  - Regulatory Body: European Insurance Authority\n",
      "\n",
      "- **Instance:** European_Council (Class: Institution)\n",
      "  - Regulatory Body: Directive 2009/138/EC\n",
      "\n",
      "- **Instance:** collective_investment_undertakings (Class: Instrument)\n",
      "\n",
      "- **Instance:** qualifying_social_entrepreneurship_funds (Class: Instrument)\n",
      "  - Article: 3(b) of Regulation (EU) No 346/2013\n",
      "\n",
      "- **Instance:** qualifying_venture_capital_funds (Class: Instrument)\n",
      "  - Article: 3(b) of Regulation (EU) No 345/2013\n",
      "\n",
      "- **Instance:** closed_ended_unleveraged_AIF (Class: Instrument)\n",
      "  - Article: 35 or 40 of Directive 2011/61/EU\n"
     ]
    }
   ],
   "source": [
    "with open('part_ii/equity_risk_submodule.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "main(onto, single_question=\"Using the given ontology as a framework, convert from the following text, into instances of the ontology. Provide your answer in points. Make sure to grab institutions or regulatory bodies. Text: \"+text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "\n",
    "We note that the quality of the output can naturally differ depending on the LLM used. Furthermore, in our example, we have provided only a simple ontology that has returned acceptable results in classifying new text under the ontological framework. With even more examples, one would be able to fine-tune an LLM to better classify new documents to help populate and develop their ontology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Outlook\n",
    "\n",
    "Here we have provided a very simple implementation of feeding an LLM with an ontology.\n",
    "However, it is clear that given the structure of an ontology, it helps LLMs to grab contextually relevant information.\n",
    "\n",
    "This code demonstrates how ontologies and large language models can be effectively integrated to enhance knowledge retrieval and contextual understanding. By reading an ontology from an OWL file, the script extracts its structure, including classes, properties, and instances, and transforms this information into structured text which is easier for the LLM to understand the nuances of our real world requirements.\n",
    "\n",
    "However, with even more data and computational resouces, ontologies can be used with LLMs in several more powerful ways to improve knowledge retrieval, reasoning, and consistency such as:\n",
    "\n",
    "1. Knowledge Enhancement (Retrieval Augmented Generation)\n",
    "Ontologies structure domain knowledge, which can be retrieved and provided as context to an LLM. (Which we have scratched the surface of conceptually)\n",
    "2. Better Understanding of Queries\n",
    "LLMs can use ontologies to disambiguate terms (e.g., “bank” as a financial institution vs. a riverbank).\n",
    "Helps interpret user intent more precisely by mapping terms to concepts.\n",
    "3. Reasoning and Consistency Checking\n",
    "Ontologies define rules (e.g., “All mammals have lungs”).\n",
    "LLMs can use these rules to verify facts and ensure consistent answers.\n",
    "4. Data Integration and Structuring\n",
    "LLMs can generate structured outputs (like JSON or RDF) based on ontology definitions.\n",
    "Example: If an ontology defines \"Patient → hasCondition → Disease,\" an LLM can generate structured knowledge graphs.\n",
    "5. Fine-tuning LLMs\n",
    "Instead of using general knowledge, an LLM can be fine-tuned on an ontology’s data to specialize in a specific field (e.g., finance, healthcare, energy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[[1] Ontology Wiki](https://en.wikipedia.org/wiki/Ontology_(information_science))\\\n",
    "[[2] Ontology vs. KG](https://enterprise-knowledge.com/whats-the-difference-between-an-ontology-and-a-knowledge-graph/)\\\n",
    "[[3] ESG Ontology](https://www.mdpi.com/2079-9292/13/9/1719)\\\n",
    "[[4] SCR](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32015R0035)\\\n",
    "[[5] Owlready2](https://owlready2.readthedocs.io/en/latest/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
